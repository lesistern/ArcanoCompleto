# ğŸš« Problema: Scraping Bloqueado por 403 Forbidden

El sitio dndtools.org estÃ¡ bloqueando requests automatizados con error 403 Forbidden. Esto es una medida de protecciÃ³n comÃºn contra scrapers.

## ğŸ›¡ï¸ Por quÃ© estÃ¡ bloqueado

1. **User-Agent Detection**: El servidor detecta que no es un navegador real
2. **Rate Limiting**: ProtecciÃ³n contra scraping masivo
3. **JavaScript Required**: El sitio puede requerir JavaScript para cargar contenido
4. **Cloudflare/Protection**: Puede estar detrÃ¡s de un WAF (Web Application Firewall)

---

## âœ… Soluciones Alternativas

### OpciÃ³n 1: Usar Navegador Headless (Playwright/Puppeteer) â­ RECOMENDADO

Playwright simula un navegador real, evitando la mayorÃ­a de bloqueos.

#### Instalar Playwright

```bash
cd dnd-compendium
npm install playwright
```

#### Script con Playwright

CrearÃ© un nuevo scraper que use Playwright...

### OpciÃ³n 2: Descarga Manual + Procesamiento

1. **Descargar pÃ¡ginas manualmente** usando tu navegador
2. **Guardar HTML** en `scraped-data/raw/`
3. **Procesar HTML** con nuestros parsers

#### Ejemplo:

```bash
# 1. Abre en tu navegador:
#    https://srd.dndtools.org/srd/classes/baseCore/barbarian.html
#
# 2. Guarda como: scraped-data/raw/barbarian.html
#
# 3. Procesa:
node scripts/parse-html.mjs --file scraped-data/raw/barbarian.html
```

### OpciÃ³n 3: Usar ExtensiÃ³n de Navegador

Usar una extensiÃ³n como "Web Scraper" o "Data Miner" que scrape desde tu navegador real.

### OpciÃ³n 4: API No Oficial

Buscar si existe una API no oficial o dataset existente:

- **GitHub**: Buscar "d&d 3.5 dataset json"
- **Kaggle**: Datasets de D&D
- **Archive.org**: Versiones archivadas del SRD

---

## ğŸš€ SoluciÃ³n Inmediata: Playwright Scraper

Voy a crear un scraper mejorado con Playwright que evita el bloqueo 403.

### Ventajas de Playwright:

âœ… Navegador real (Chromium/Firefox/WebKit)
âœ… JavaScript ejecutado
âœ… Cookies y localStorage
âœ… Evita la mayorÃ­a de anti-bot protections
âœ… Screenshots para debugging

### Desventajas:

âŒ MÃ¡s lento que HTTP puro
âŒ Consume mÃ¡s recursos
âŒ Requiere dependencia adicional (~300MB)

---

## ğŸ“Š Datos Ya Disponibles

Mientras tanto, tenemos datos de mÃºltiples fuentes:

### âœ… Ya Migrados a Supabase:

- 16 razas completas
- 72 armas
- 43 skills
- 34 feats bÃ¡sicas
- 85 libros catalogados

### ğŸ“š Datos de Referencia Disponibles:

El agente anterior ya investigÃ³ y documentÃ³:
- **11 clases base** completas en `CLASES_PLAYER_HANDBOOK.md`
- **JSON listo** para migraciÃ³n en `classes-player-handbook.json`

Estos datos fueron extraÃ­dos manualmente del SRD y estÃ¡n listos para insertar.

---

## ğŸ¯ Plan de AcciÃ³n Recomendado

### Inmediato (Hoy):

1. âœ… Instalar Playwright
2. âœ… Crear scraper con Playwright
3. âœ… Scrapear categorÃ­as prioritarias:
   - Clases base (11)
   - Razas faltantes (~30)
   - Skills faltantes (si hay)

### Corto Plazo (Esta Semana):

4. Scrapear con Playwright (modo lento, respetuoso):
   - Conjuros (~3,000) - 1-2 horas
   - Dotes (~1,500) - 1 hora
   - Monstruos (~1,500) - 1 hora

### Mediano Plazo (PrÃ³ximas Semanas):

5. Scrapear resto del contenido:
   - Clases de prestigio (~100)
   - Objetos mÃ¡gicos (~2,000)
   - Equipment variado

---

## ğŸ”’ Consideraciones Ã‰ticas

Si usamos Playwright para scrapear:

1. **Rate Limiting**: Delay de 1-2 segundos entre pÃ¡ginas
2. **Horarios**: Scrapear en horarios de baja demanda
3. **PropÃ³sito**: Solo para uso personal/educativo
4. **CachÃ©**: Guardar todo localmente, no re-scrapear
5. **Respeto**: Si el sitio lo prohÃ­be explÃ­citamente, buscar alternativas

---

## ğŸ“ PrÃ³ximo Paso

Â¿QuÃ© prefieres?

**A)** Instalar Playwright y crear scraper robusto (RECOMENDADO)
**B)** Descargar pÃ¡ginas manualmente y procesarlas
**C)** Buscar datasets existentes en GitHub/Kaggle
**D)** Usar los datos ya investigados y continuar manualmente

---

**RecomendaciÃ³n**: OpciÃ³n A con Playwright, scrapeando despacio y respetuosamente, guardando todo en cachÃ© local para no volver a scrape.
